{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 20 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is the underlying concept of Support Vector Machines ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e9672-afeb-498f-9c64-9b65fdbafe97",
   "metadata": {},
   "source": [
    "**Ans** Support Vector Machines (SVMs) are a type of supervised learning algorithm used for classification and regression tasks. The underlying concept of SVMs revolves around finding the optimal hyperplane that best separates data points belonging to different classes in a high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is the concept of a support vector ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45042b56-867d-4145-a29b-2d779be2c8a4",
   "metadata": {},
   "source": [
    "**Ans** A support vector is a data point that is closest to the decision boundary (hyperplane) between different classes in a support vector machine (SVM). These are the critical data points that help define the position and orientation of the hyperplane.\n",
    "\n",
    "Support vectors are essential because they directly influence the placement of the decision boundary. In fact, the decision boundary is entirely determined by these support vectors in an SVM. Other data points, which are not support vectors, do not affect the decision boundary and are irrelevant for defining it.\n",
    "\n",
    "Support vectors are crucial for SVM because they represent the most challenging examples to classify accurately. If any of these support vectors were moved or removed, the position of the decision boundary would likely change. This emphasizes the robustness of SVM models, as they are primarily influenced by the most challenging data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. When using SVMs, why is it necessary to scale the inputs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10bf18-b46d-4783-b7be-019fc512c62a",
   "metadata": {},
   "source": [
    "**Ans** Scaling the inputs is necessary when using Support Vector Machines (SVMs) for several reasons:\n",
    "\n",
    "1. Sensitive to Feature Scales: SVMs are sensitive to the scale of input features. Since the objective of SVM is to find the optimal hyperplane that best separates classes, the scale of features can significantly affect the decision boundary. Features with larger scales can dominate those with smaller scales, leading to a biased model.\n",
    "2. Euclidean Distance: Many SVM algorithms, especially those based on the kernel trick, rely on measuring distances between data points. If the features are not scaled, features with larger scales will have a larger impact on distance computations, potentially distorting the geometric relationships between data points.\n",
    "3. Regularization: In SVM, regularization is often applied to control the trade-off between maximizing the margin and minimizing the classification error. The regularization parameter (C) can have different effects on features with different scales if they are not normalized. Scaling the inputs ensures that the regularization parameter has a similar effect on all features.\n",
    "4. Convergence Speed: Scaling features can also help improve the convergence speed of optimization algorithms used to train SVM models. Features with larger scales may lead to slower convergence or numerical instability during optimization.\n",
    "5. Kernel Functions: When using kernel functions in SVM, such as the radial basis function (RBF) kernel, scaling is particularly important. The RBF kernel computes the similarity between data points based on their Euclidean distances. If features are not scaled, the notion of similarity between data points may become distorted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607e802-50fa-4324-b1ad-f78f654ae170",
   "metadata": {},
   "source": [
    "**Ans** Yes, SVM classifiers can provide a confidence score or a measure of certainty regarding their predictions, but they do not inherently produce probability estimates like some other classifiers such as logistic regression or decision trees. However, there are methods to approximate confidence scores or probabilities from SVM outputs.\n",
    "\n",
    "Yes, SVM classifiers can provide a confidence score or a measure of certainty regarding their predictions, but they do not inherently produce probability estimates like some other classifiers such as logistic regression or decision trees. However, there are methods to approximate confidence scores or probabilities from SVM outputs.\n",
    "\n",
    "Here are a couple of common approaches:\n",
    "\n",
    "1. Distance from Hyperplane: SVM classifiers assign a class label based on which side of the decision boundary (hyperplane) the data point lies. The distance of a data point from the hyperplane can serve as a measure of confidence. Typically, the further away a point is from the decision boundary, the more confident the classifier is in its prediction. However, this distance alone does not directly translate into a percentage chance or probability.\n",
    "2. Platt Scaling: Platt scaling is a technique used to calibrate the output of SVM classifiers to approximate probabilities. It involves fitting a logistic regression model to the SVM decision values (raw output scores) and using the logistic function to transform these values into probabilities. Platt scaling requires a separate calibration dataset or cross-validation procedure to estimate the parameters of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99850de3-f9dc-4432-92e0-1b29f0b3d416",
   "metadata": {},
   "source": [
    "**Ans** Given the dataset has millions of instances and hundreds of features, the choice between the primal and dual form may depend on the specific characteristics of your dataset and your computational resources.\n",
    "\n",
    "1. If the dataset is sparse (i.e., most feature values are zero), the primal form may be more efficient due to its memory efficiency and ability to handle sparse data.\n",
    "2. If the dataset is dense and has a relatively small number of features compared to the number of instances, the dual form might be more appropriate due to its computational efficiency in high-dimensional spaces.\n",
    "\n",
    "It's also worth considering implementation details and available libraries. Some SVM implementations may handle large-scale datasets more efficiently in one form over the other. Therefore, it's advisable to experiment with both forms and choose the one that provides the best balance of computational efficiency and memory usage for your specific dataset and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cafabb-6690-4213-98c2-4725aafc9575",
   "metadata": {},
   "source": [
    "**Ans** If your SVM classifier with an RBF kernel underfits the training data, you can experiment with raising gamma and/or lowering C to potentially improve its performance.\n",
    "\n",
    "However, it's crucial to perform proper validation using a separate validation set or cross-validation to assess the impact of these hyperparameter changes on the model's performance on unseen data and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ac2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f716511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC coefficients: [[-0.8458694   2.72214092]]\n",
      "LinearSVC intercept: [0.68229091]\n",
      "SVC coefficients: [[-0.85436812  2.74083787]]\n",
      "SVC intercept: [0.69382653]\n",
      "SGDClassifier coefficients: [[0.14565935 3.51557756]]\n",
      "SGDClassifier intercept: [0.93960535]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anacondaInstalledHere\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "linear_svc = LinearSVC(loss='hinge', random_state=42)\n",
    "linear_svc.fit(X_scaled, y)\n",
    "\n",
    "svc_linear = SVC(kernel='linear', random_state=42)\n",
    "svc_linear.fit(X_scaled, y)\n",
    "\n",
    "sgd_clf = SGDClassifier(loss='hinge', random_state=42)\n",
    "sgd_clf.fit(X_scaled, y)\n",
    "\n",
    "print(\"LinearSVC coefficients:\", linear_svc.coef_)\n",
    "print(\"LinearSVC intercept:\", linear_svc.intercept_)\n",
    "\n",
    "print(\"SVC coefficients:\", svc_linear.coef_)\n",
    "print(\"SVC intercept:\", svc_linear.intercept_)\n",
    "\n",
    "print(\"SGDClassifier coefficients:\", sgd_clf.coef_)\n",
    "print(\"SGDClassifier intercept:\", sgd_clf.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d926bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anacondaInstalledHere\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "E:\\anacondaInstalledHere\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\anacondaInstalledHere\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\anacondaInstalledHere\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\anacondaInstalledHere\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', decision_function_shape='ovr')\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=3, scoring='precision_macro')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_svm_model = SVC(kernel='rbf', decision_function_shape='ovr', C=best_params['C'], gamma=best_params['gamma'])\n",
    "best_svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = best_svm_model.predict(X_test_scaled)\n",
    "precision = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\")\n",
    "print(precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. On the California housing dataset, train an SVM regressor ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc128984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.35700264267544685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(california_housing.data, california_housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Choose an SVM regressor model\n",
    "svm_regressor = SVR(kernel='rbf')\n",
    "\n",
    "# Train the SVM regressor\n",
    "svm_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2614bb6-4a33-4f74-87cb-a91a3d99c4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
